{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a7513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyswarms as ps\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20452297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donee\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"file_path.csv\")\n",
    "\n",
    "# CHANGE THE FILE PATH!\n",
    "# Extract features and target\n",
    "X = data.iloc[:, :-1].values  # the Features\n",
    "y = data.iloc[:, -1].values   # Target\n",
    "print(\"donee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de60997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Network architecture parameters\n",
    "n_inputs = X.shape[1]\n",
    "n_hidden = 20\n",
    "n_classes = len(np.unique(y))\n",
    "num_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd33d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_function(p):\n",
    "    \"\"\" Calculate roll-back the weights and biases \"\"\"\n",
    "    #edited\n",
    "    W1_end = n_inputs * n_hidden\n",
    "    b1_end = W1_end + n_hidden\n",
    "    W2_end = b1_end + n_hidden * n_classes\n",
    "    \n",
    "    W1 = p[:W1_end].reshape((n_inputs, n_hidden))\n",
    "    b1 = p[W1_end:b1_end].reshape((n_hidden,))\n",
    "    W2 = p[b1_end:W2_end].reshape((n_hidden, n_classes))\n",
    "    b2 = p[W2_end:].reshape((n_classes,))\n",
    "    \n",
    "    # Perform forward propagation\n",
    "    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n",
    "    a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    logits = a1.dot(W2) + b2  # Pre-activation in Layer 2\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed3b5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_prop(params):\n",
    "    \"\"\"Forward propagation as objective function\n",
    "\n",
    "    This computes for the forward propagation of the neural network, as\n",
    "    well as the loss.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    params: np.ndarray\n",
    "        The dimensions should include an unrolled version of the\n",
    "        weights and biases.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The computed negative log-likelihood loss given the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    logits = logits_function(params)\n",
    "\n",
    "    # Compute for the softmax of the logits\n",
    "    exp_scores = np.exp(logits)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute for the negative log likelihood\n",
    "\n",
    "    corect_logprobs = -np.log(probs[range(num_samples), y])\n",
    "    loss = np.sum(corect_logprobs) / num_samples\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3471022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Higher-level method to do forward_prop in the\n",
    "    whole swarm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
    "        The swarm that will perform the search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of shape (n_particles, )\n",
    "        The computed loss for each particle\n",
    "    \"\"\"\n",
    "    #edited\n",
    "    n_particles = x.shape[0]\n",
    "    loss = np.zeros(n_particles)\n",
    "    \n",
    "    for i in range(n_particles):\n",
    "        logits = logits_function(x[i])\n",
    "        y_pred = np.argmax(logits, axis=1)\n",
    "        loss[i] = np.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b79e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 00:37:47,901 - pyswarms.single.global_best - INFO - Optimize for 1000 iters with {'c1': 0.6, 'c2': 0.4, 'w': 0.8}\n",
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████|1000/1000, best_cost=0.134\n",
      "2024-05-18 00:38:37,005 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.13361344537815126, best pos: [ 1.92979059  0.96089478 -0.62751143  2.42742664  0.21502578  1.91182621\n",
      "  1.7990874   0.49831577  0.61710886  1.57870634  2.59794742  0.92987451\n",
      "  1.00063124  2.03105391  1.11941467  1.26020228  1.27634105  2.48740745\n",
      "  2.33265249  0.6336027   0.88834534  2.55105064  1.33708542  1.4708917\n",
      "  1.093855    0.71322119 -1.43602411  1.49916656  1.74628358 -0.78386571\n",
      "  0.93732691  1.14659392  0.88720549  1.50277252 -0.86872051  1.41805894\n",
      "  1.77497787  1.80360886  0.61970869  0.39436206  1.46143389  0.06612377\n",
      "  1.7718036   2.47801772  2.58827435  0.44026289  1.84152347  1.45172466\n",
      " -0.16213305  0.42327081  1.2735766  -0.32495434  1.84497077  0.61031625\n",
      "  0.58703712  0.24907638  1.69931712  1.66699778  0.84276247  2.20628719\n",
      "  1.38672878  1.86076593  1.79034031  1.2676188   1.21061258  1.42250632\n",
      "  1.05757937  0.34169728  0.28292454  0.53204868  1.58980205  0.43288631\n",
      "  1.18542454  1.07553461  1.60080896  0.72957435  0.80023869  0.84734894\n",
      "  2.25306826  1.58739692  1.89720425  1.17447384  0.96856276  1.53370647\n",
      " -0.25167787  0.10152455  0.69904229  1.14522306  0.8210878   0.21236816\n",
      "  2.04805631  1.45223615  1.63215213  1.73439008  2.01713733 -0.93589382\n",
      "  0.8720847   1.95622686  1.7892972   2.29713122  1.31390391  0.5991926\n",
      "  0.72010229  0.89238136  0.0576829   1.33083735  1.60355049  1.59615367\n",
      "  1.21014288  0.49100301  1.54336771  1.04904668  0.77010269  0.67303106\n",
      "  0.79699256  0.87592724  2.36755136  2.33452593  1.35336208 -0.10047467\n",
      "  0.90848152  0.39542014  1.5513535   1.09832297  1.00912424  1.36374329\n",
      " -0.6377533   1.66662526  1.52467915  1.29328499  1.69843799  1.97105297\n",
      " -0.01192878  0.86606689  1.84365238 -0.04555844  1.85066445  0.31841025\n",
      "  1.57871383  1.25507533  0.79038476  1.67962992  0.82855401  1.20552923\n",
      "  0.01042773 -0.21857631  1.43694063  0.19273131  1.82191724  0.39626594\n",
      "  0.85898634  0.23357915  1.8492382   1.96387717  0.732746    0.22446417\n",
      " -0.13089449  2.00338235  1.9497012   1.34469376  0.47593957  0.72257988\n",
      "  1.7347864   1.93732362 -0.08806388  0.53839301  0.93622042  1.23841587\n",
      "  0.96121548  1.39950806  2.74799115  1.15027641  0.79017257  2.22127283\n",
      "  0.87128343  0.90461122  0.10037065  0.31106638  0.7321362   2.45346182\n",
      "  0.43292906  1.34704921  1.21833497  0.28802568  1.3695558   0.51285832\n",
      "  1.54557849  1.32875953  1.14395106  1.37915534  0.64605801  0.31354644\n",
      "  0.84283417  0.65195613  0.98325463  1.3309395   0.61812158  1.4341244\n",
      "  1.51435051  0.8917283   0.750104    1.33487099  0.82796016  1.09714055\n",
      "  1.38731898  0.75392796  1.10057506  0.7783934   1.1059489   1.08206257\n",
      " -0.06242915  1.87311675  1.33066202 -0.11486747  1.62103502  1.70092111\n",
      "  1.30245651  1.69992968  0.96901571  1.1702266   0.79552528  2.58402022\n",
      "  0.9268441   1.05183518  0.11264668  1.4446523   0.55688803  1.56983617\n",
      "  0.39520186  0.37603446  0.37100062  1.09871735  1.63511933  1.88904206\n",
      "  1.12385054  0.45165955  1.80837271 -0.18805388  2.73976884  1.58530235\n",
      "  0.66422121  0.44264626  0.76433499  0.58909207 -0.39252621 -0.0445725\n",
      "  0.99604439  1.59363997  1.33772325  2.51968624 -0.09350319  0.24242766\n",
      "  1.8525044   0.53626014 -0.04906869  0.64213683  1.90904617  1.8209739\n",
      "  1.82397329  2.58308546  1.7849692   0.97352918  0.00388055  1.55039436\n",
      "  0.6555261   0.67062145  1.84561316  0.75095752  2.56741171  0.19223465\n",
      " -1.07018246  0.71505936  1.02470687  0.84346682  0.6689272   1.92729785\n",
      "  0.97981377  0.11407769  0.67372658  1.50445569  1.54603267  1.83406796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 50s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize swarm\n",
    "#edited cuz higher accuracy\n",
    "options = {'c1': 0.6, 'c2': 0.4, 'w':0.8}\n",
    "#options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
    "# Call instance of PSO\n",
    "\n",
    "dimensions = (n_inputs * n_hidden) + (n_hidden * n_classes) + n_hidden + n_classes\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=100, dimensions=dimensions, options=options)\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(f, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433f7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pos):\n",
    "    \"\"\"\n",
    "    Use the trained weights to perform class predictions.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    pos: numpy.ndarray\n",
    "        Position matrix found by the swarm. Will be rolled\n",
    "        into weights and biases.\n",
    "    \"\"\"\n",
    "    logits = logits_function(pos)\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4fcc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.8663865546218488\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = (predict(pos) == y).mean()\n",
    "print(f'Model accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e3e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1ea44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf7fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e9a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
